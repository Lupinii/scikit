{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac31af4b-6b18-4f7a-a8a0-abfcf2ed423b",
   "metadata": {},
   "source": [
    "<h3>Scikit-Learn</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5f96f-7bd7-4218-b282-ab65fd2538d5",
   "metadata": {},
   "source": [
    "<h3 style='color:green;'>Pipelines</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ae3f6a-721a-4f70-9de1-9e4ec504dd55",
   "metadata": {},
   "source": [
    "We are discussing preprocessing pipelines in machine learning.\n",
    "\n",
    "Preprocessing pipelines are a way to organize and automate the sequence of data transformations that are applied to raw data before feeding it into a machine learning model.\n",
    "\n",
    "They are important for ensuring that the same steps are applied consistently to both training and testing data, and they help in making the code more readable and maintainable.\n",
    "\n",
    "Steps in a typical preprocessing pipeline might include:\n",
    "\n",
    "1. Handling missing values (imputation)\n",
    "\n",
    "2. Encoding categorical variables (one-hot, label encoding, etc.)\n",
    "\n",
    "3. Scaling or normalizing numerical features\n",
    "\n",
    "4. Feature engineering (creating new features)\n",
    "\n",
    "5. Feature selection\n",
    "\n",
    "In Python, the `sklearn.pipeline` module provides utilities to build such pipelines. The key components are:\n",
    "\n",
    "- Transformers: Objects that implement `fit` and `transform` (or `fit_transform`) to clean, reduce, expand, or generate features.\n",
    "\n",
    "- Estimators: Typically a machine learning model that implements `fit` and `predict`.\n",
    "\n",
    "- Pipeline: A pipeline chains multiple transformers and a final estimator.\n",
    "\n",
    "Example of a simple pipeline:\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define preprocessing for numeric columns (impute missing values and scale them)\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "\n",
    "('imputer', SimpleImputer(strategy='mean')),\n",
    "\n",
    "('scaler', StandardScaler())])\n",
    "\n",
    "# Define preprocessing for categorical columns (impute missing values and one-hot encode)\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "\n",
    "('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "\n",
    "('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "\n",
    "transformers=[\n",
    "\n",
    "('num', numeric_transformer, ['age', 'income']),\n",
    "\n",
    "('cat', categorical_transformer, ['gender', 'city'])])\n",
    "\n",
    "# Create a pipeline that includes the preprocessor and a classifier\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "\n",
    "('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Now you can use this pipeline as a single estimator\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "Advantages of using pipelines:\n",
    "\n",
    "- Avoid data leakage: Since each step is fitted only on the training data during cross-validation, and the test data is transformed without being fitted again.\n",
    "\n",
    "- Convenience: You can treat the entire sequence of steps as a single unit, making it easier to deploy and share.\n",
    "\n",
    "- Reproducibility: Ensures the same steps are applied in the same order every time.\n",
    "\n",
    "- Grid search: You can perform hyperparameter tuning over all steps of the pipeline simultaneously.\n",
    "\n",
    "Note: The `ColumnTransformer` is particularly useful when you want to apply different transformations to different columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7dd2bc-8057-4986-8cae-59d6a229aa67",
   "metadata": {},
   "source": [
    "In machine learning, a preprocessing pipeline is a structured sequence of data transformation steps applied to raw input data before feeding it to a model. Pipelines automate and standardize preprocessing, ensuring consistency, efficiency, and reproducibility. Here's a breakdown:\n",
    "\n",
    "Why Use Pipelines?\n",
    "Avoid Data Leakage:\n",
    "Preprocessing steps (e.g., scaling) are fitted only on training data, preventing test/validation data from influencing transformations.\n",
    "\n",
    "Code Simplicity:\n",
    "Bundle all preprocessing steps and model training into a single object.\n",
    "\n",
    "Reproducibility:\n",
    "Apply identical transformations during training, testing, and deployment.\n",
    "\n",
    "Hyperparameter Tuning:\n",
    "Optimize preprocessing parameters alongside model parameters in a single grid search.\n",
    "\n",
    "Core Components\n",
    "1. Data Cleaning\n",
    "Handle missing values (e.g., SimpleImputer in scikit-learn).\n",
    "\n",
    "Remove duplicates or irrelevant samples/features.\n",
    "\n",
    "2. Feature Engineering\n",
    "Create new features (e.g., polynomial features, date/time decompositions).\n",
    "\n",
    "Binning numerical data or aggregating categories.\n",
    "\n",
    "3. Categorical Encoding\n",
    "Convert categorical variables into numerical formats:\n",
    "\n",
    "One-Hot Encoding (OneHotEncoder).\n",
    "\n",
    "Ordinal Encoding (OrdinalEncoder).\n",
    "\n",
    "Target Encoding (TargetEncoder).\n",
    "\n",
    "4. Scaling & Normalization\n",
    "Standardize numerical features to similar scales:\n",
    "\n",
    "Standardization (StandardScaler: mean=0, variance=1).\n",
    "\n",
    "Min-Max Scaling (MinMaxScaler: scale to [0, 1]).\n",
    "\n",
    "Robust Scaling (RobustScaler: resistant to outliers).\n",
    "\n",
    "5. Dimensionality Reduction (Optional)\n",
    "PCA, t-SNE, or feature selection to reduce noise/overfitting.\n",
    "\n",
    "6. Model Training\n",
    "Final step: pass preprocessed data to an estimator (e.g., classifier/regressor)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d84741-9c4a-474f-9ad6-2bd6f3661ea3",
   "metadata": {},
   "source": [
    "Implementation Example (scikit-learn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7ebed6-fac8-4c5d-975a-364d5626fde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define preprocessing for different feature types\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, ['age', 'income']),\n",
    "        ('cat', categorical_transformer, ['gender', 'city'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Full pipeline: preprocessor + model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Train and predict\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcea3bd5-0674-41df-96c8-024317919a41",
   "metadata": {},
   "source": [
    "Key Benefits\n",
    "Consistency: Same transformations applied to all data splits.\n",
    "\n",
    "Modularity: Easily add/remove steps (e.g., swap StandardScaler for RobustScaler).\n",
    "\n",
    "Deployment: Deploy a single pipeline object (no manual preprocessing in production).\n",
    "\n",
    "Cross-Validation: Pipelines work seamlessly with cross_val_score or GridSearchCV.\n",
    "\n",
    "Tools & Libraries\n",
    "scikit-learn: Pipeline, ColumnTransformer, FunctionTransformer.\n",
    "\n",
    "imbalanced-learn: Pipelines for handling class imbalance (e.g., SMOTE).\n",
    "\n",
    "Feature-engine: Dedicated library for feature engineering pipelines.\n",
    "\n",
    "Best Practices\n",
    "Order Matters: Impute before scaling, encode before modeling.\n",
    "\n",
    "Column-Specific Transformations: Use ColumnTransformer for different feature types.\n",
    "\n",
    "Custom Transformers: Create reusable classes for bespoke steps (e.g., log transformations).\n",
    "\n",
    "Summary\n",
    "Preprocessing pipelines encapsulate the end-to-end workflow of transforming raw data into model-ready inputs. They mitigate human error, enhance reproducibility, and streamline model development and deployment. By integrating preprocessing with model training, pipelines ensure robustness and scalability in ML projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61cc982-db7c-4670-85a6-6aac961e2ee3",
   "metadata": {},
   "source": [
    "<h3 style='color:green;'>Data Splitting</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5c3edc-cd6d-4362-af2e-847af1a4f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "We are going to discuss how to split data in machine learning.\n",
    "\n",
    "Typically, we split the data into training, validation, and test sets.\n",
    "\n",
    "However, sometimes we only split into training and test sets, and then further split the training set for validation (e.g., via cross-validation).\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Why split? To evaluate model performance on unseen data and avoid overfitting.\n",
    "\n",
    "2. Common splits:\n",
    "\n",
    "- Training set: used to train the model.\n",
    "\n",
    "- Validation set: used to tune hyperparameters and evaluate the model during training.\n",
    "\n",
    "- Test set: used only once at the end to assess the final model's performance.\n",
    "\n",
    "3. Typical proportions:\n",
    "\n",
    "- 60-20-20 (train, validation, test) or 80-20 (train and test, then use cross-validation on the training for validation)\n",
    "\n",
    "- Also common: 70-15-15, or 80-10-10, depending on data size.\n",
    "\n",
    "4. How to split:\n",
    "\n",
    "- Random splitting (using scikit-learn's `train_test_split` for example)\n",
    "\n",
    "- Stratified splitting (to preserve the same distribution of classes in each set, especially for classification with imbalanced classes)\n",
    "\n",
    "- Time-based splitting (if the data is time-series, we must respect the temporal order)\n",
    "\n",
    "5. Implementation in Python using scikit-learn:\n",
    "\n",
    "Example 1: Two splits (train and test)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "Example 2: Three splits (train, validation, test)\n",
    "\n",
    "# First split: 80% train, 20% temp\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then split the temp (which is 20% of original) into validation and test (each 10% of original)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "Alternatively, we can use `train_test_split` twice to get three sets.\n",
    "\n",
    "Example 3: Using `train_test_split` with stratification\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "6. Cross-validation:\n",
    "\n",
    "- Instead of a single validation set, we split the training data into k folds and use each fold as a validation set while training on the rest.\n",
    "\n",
    "- Example: k=5 (5-fold cross-validation)\n",
    "\n",
    "7. Important:\n",
    "\n",
    "- The test set should never be used for any training or parameter tuning.\n",
    "\n",
    "- Set a random seed (random_state) for reproducibility.\n",
    "\n",
    "8. For time series data:\n",
    "\n",
    "- We cannot split randomly. Instead, we use a cutoff time.\n",
    "\n",
    "- Example: Use data up to time T for training, from T to T' for validation, and after T' for testing.\n",
    "\n",
    "9. Libraries that help:\n",
    "\n",
    "- scikit-learn: `train_test_split`, `TimeSeriesSplit`, `StratifiedKFold`, etc.\n",
    "\n",
    "- Also, `sklearn.model_selection` has `ShuffleSplit`, `StratifiedShuffleSplit`, etc.\n",
    "\n",
    "10. Advanced:\n",
    "\n",
    "- Grouped splits: when data points are grouped (e.g., multiple samples from the same patient) and we want to keep groups entirely in one set (use `GroupKFold` or `GroupShuffleSplit`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34556a5a-e708-4c71-b2c4-f0ed3c1032d8",
   "metadata": {},
   "source": [
    "1. Standard Data Splits\n",
    "Train-Validation-Test Split\n",
    "\n",
    "Training Set: Used to train the model (60-80% of data).\n",
    "\n",
    "Validation Set: Used to tune hyperparameters and monitor training (10-20%).\n",
    "\n",
    "Test Set: Used only once for final unbiased evaluation (10-20%).\n",
    "\n",
    "Common Ratios:\n",
    "\n",
    "70-15-15 (moderate-sized data)\n",
    "\n",
    "80-10-10 (large datasets)\n",
    "\n",
    "60-20-20 (small datasets)\n",
    "\n",
    "Train-Test Split\n",
    "Use when validation is handled via cross-validation (e.g., smaller datasets).\n",
    "\n",
    "Training: 70-80%\n",
    "\n",
    "Test: 20-30%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d08520-def3-464c-b751-57f6f72461c8",
   "metadata": {},
   "source": [
    "2. Key Splitting Strategies\n",
    "Random Splitting:\n",
    "Shuffle data randomly before splitting. Use for independent, identically distributed (IID) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2ffc57-bc23-439a-9f6f-d7cef7cfaa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-Test Split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train-Validation-Test Split (70/15/15)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29efe04a-49e4-413a-89ce-a86cbdf7df02",
   "metadata": {},
   "source": [
    "Stratified Splitting:\n",
    "Preserve class distribution in each split (critical for imbalanced datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28b968-05a4-4af0-87f6-a7d94e43af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a212a9b-bc67-4e6b-9cfd-d90304899187",
   "metadata": {},
   "source": [
    "Time-Based Splitting:\n",
    "Use for time-series data to avoid future leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f005f0-dfdd-4921-8b89-fed5a7f14e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = int(0.8 * len(data))  # 80% train, 20% test\n",
    "X_train, y_train = X[:cutoff], y[:cutoff]\n",
    "X_test, y_test = X[cutoff:], y[cutoff:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af1d5b4-92bd-43c4-88cd-ba8173a64d79",
   "metadata": {},
   "source": [
    "Grouped Splitting:\n",
    "Keep related samples (e.g., same patient) together to avoid leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c36276-5472-49d5-ac52-9f938dc7691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(splitter.split(X, y, groups=patient_ids))\n",
    "X_train, X_test = X[train_idx], X[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7dfe57-7782-459b-8850-3383c06f926a",
   "metadata": {},
   "source": [
    "3. Cross-Validation (Advanced Splitting)\n",
    "Use for small datasets or maximizing data usage:\n",
    "\n",
    "k-Fold Cross-Validation:\n",
    "Split data into k folds. Train on k-1 folds, validate on the remaining fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db61e57-f279-4e50-abcb-630ce400f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc676975-bc87-4221-a6bb-969476a1715c",
   "metadata": {},
   "source": [
    "Stratified k-Fold:\n",
    "Preserve class distribution in each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e95c17-bb5c-4917-8d2e-00570bafe3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c534af-6305-4eee-a7ea-17578535354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Best Practices\n",
    "Avoid Test Set Contamination:\n",
    "Never use the test set for training, validation, or feature engineering.\n",
    "\n",
    "Set Random Seeds: Ensure reproducibility (e.g., random_state=42).\n",
    "\n",
    "Large Datasets: Reduce validation/test size (e.g., 200k samples → 1% test).\n",
    "\n",
    "Small Datasets: Prefer cross-validation over fixed splits.\n",
    "\n",
    "Data Leakage:\n",
    "\n",
    "Split data before preprocessing (e.g., scaling).\n",
    "\n",
    "Use pipelines (e.g., sklearn.pipeline.Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e6e3bd-6ea0-4fd1-b03d-7f6c33db24f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. When to Use Which Split?\n",
    "Scenario\tRecommended Split\n",
    "Large IID data\tRandom 80/10/10\n",
    "Imbalanced classes\tStratified split\n",
    "Time-series data\tTime-based split\n",
    "Small datasets (<10k samples)\tk-Fold Cross-Validation\n",
    "Grouped data (e.g., patients)\tGrouped split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe4abe-ee7a-465c-9bc8-ecfd884a698a",
   "metadata": {},
   "source": [
    "Example Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12044a9-cc7f-4daa-a423-712658758500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 2. Load data\n",
    "X, y = load_data()\n",
    "\n",
    "# 3. Initial stratified train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Further split training into train/validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, stratify=y_train, random_state=42  # 0.25 * 0.8 = 20% val\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aff1108-762c-4881-85f7-b7802f1a5b85",
   "metadata": {},
   "source": [
    "<h3 style='color:green;'>Feature Scaling Normalization vs Standardization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571cced-033a-495a-87cd-7995419d85a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "We are going to discuss feature scaling in machine learning, specifically normalization and standardization.\n",
    "\n",
    "These are preprocessing steps that are crucial for many algorithms, especially those that are distance-based or use gradient descent.\n",
    "\n",
    "Normalization (Min-Max Scaling):\n",
    "\n",
    "Rescales the features to a fixed range, usually [0, 1] or [-1, 1].\n",
    "\n",
    "Formula: X_normalized = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "Standardization (Z-score Scaling):\n",
    "\n",
    "Rescales the features to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Formula: X_standardized = (X - μ) / σ\n",
    "\n",
    "Why do we need feature scaling?\n",
    "\n",
    "Many machine learning algorithms perform better when numerical input variables are scaled.\n",
    "\n",
    "For example, algorithms that use distances between data points (like KNN, K-means, SVM) are sensitive to the scale of the features.\n",
    "\n",
    "Similarly, gradient descent-based algorithms (like linear regression, logistic regression, neural networks) converge faster when features are scaled.\n",
    "\n",
    "Steps to apply:\n",
    "\n",
    "1. Split the data into training and test sets first to avoid data leakage.\n",
    "\n",
    "2. Fit the scaler on the training data only (i.e., compute the min and max for normalization, or the mean and std for standardization on the training set).\n",
    "\n",
    "3. Transform both the training and test data using the scaler fitted on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250df850-ef79-47b0-b9cb-213b85f54c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature scaling is essential for algorithms sensitive to feature magnitude (e.g., SVM, KNN, PCA, gradient descent-based models). Here's a concise comparison:\n",
    "\n",
    "Normalization (Min-Max Scaling)\n",
    "Goal: Rescale features to a fixed range (e.g., [0, 1]).\n",
    " \n",
    "Use Cases:\n",
    "\n",
    "Algorithms requiring bounded input (e.g., neural networks).\n",
    "\n",
    "Distance-based algorithms (e.g., KNN, K-Means).\n",
    "\n",
    "When data lacks Gaussian distribution.\n",
    "\n",
    "Pros:\n",
    "\n",
    "Preserves original distribution shape.\n",
    "\n",
    "Intuitive: all features share the same scale.\n",
    "\n",
    "Cons:\n",
    "\n",
    "Sensitive to outliers (outliers compress non-outlier values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e977c65-feed-427e-95c8-0745f1d1291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))  # Or [-1, 1] if needed\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)  # Use same scaler!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf5cc1d-5df5-4051-a391-c3b3cef3ef1b",
   "metadata": {},
   "source": [
    "Standardization (Z-Score Scaling)\n",
    "Goal: Center features around mean (0) with unit variance (1).\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "Linear/Logistic Regression, SVM, PCA.\n",
    "\n",
    "When data follows (or assumes) Gaussian distribution.\n",
    "\n",
    "Gradient descent optimization.\n",
    "\n",
    "Pros:\n",
    "\n",
    "Handles outliers better than normalization.\n",
    "\n",
    "Maintains useful properties of standard deviation.\n",
    "\n",
    "Cons:\n",
    "\n",
    "Does not bound values to a fixed range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d8a18b-b20f-4fd4-b553-dd9b0025c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)  # Critical: avoid data leakage!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bad58cc-e80f-40c4-a98a-b35dd58d5158",
   "metadata": {},
   "source": [
    "Key Considerations\n",
    "Data Leakage Prevention:\n",
    "\n",
    "Always fit scalers only on training data, then transform test data.\n",
    "\n",
    "Never scale before train-test split!\n",
    "\n",
    "Algorithm Sensitivity:\n",
    "\n",
    "Use Normalization: Neural networks, KNN, K-Means, images (pixel scaling).\n",
    "\n",
    "Use Standardization: Linear models, SVM, PCA, LDA.\n",
    "\n",
    "Tree-based algorithms (e.g., Random Forest, XGBoost) generally do not require scaling.\n",
    "\n",
    "Outliers:\n",
    "\n",
    "Standardization is more robust to outliers.\n",
    "\n",
    "For normalization, clip extreme values first or use RobustScaler (scales with median/IQR).\n",
    "\n",
    "Practical Workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9507e3-e747-4fca-8d8f-1d8cf51fa9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Step 2: Scale features\n",
    "scaler = StandardScaler()  # or MinMaxScaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Apply same transformation\n",
    "\n",
    "# Step 3: Train model\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a77462a-2e58-406c-847b-46de9c192811",
   "metadata": {},
   "source": [
    "Visual Comparison\n",
    "\n",
    "Method\t         Output Range\t   Handles Outliers\t  Affected by Distribution\n",
    "Normalization\t [0, 1] or [-1,1]\t❌ Poor\t          ❌\n",
    "Standardization\t Unbounded\t        ✅ Better\t      ✅ (Assumes Gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe73340-125b-4327-8784-9e3c070d9252",
   "metadata": {},
   "source": [
    "When to Skip Scaling?\n",
    "Tree-based models: Splits depend on value ordering, not magnitude.\n",
    "\n",
    "Naive Bayes: Handles features independently.\n",
    "\n",
    "Non-gradient algorithms (e.g., Decision Trees).\n",
    "\n",
    "Rule of Thumb:\n",
    "\n",
    "\"When in doubt, standardize. If bounded ranges are critical, normalize.\"\n",
    "\n",
    "By mastering these techniques, you ensure models converge faster and perform optimally!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76186cae-7bf2-4c8f-8695-d930aa4259a9",
   "metadata": {},
   "source": [
    "<h3 style='color:green;'>Ordinal, OneHot and Label Encoders</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8645f98c-d0f7-4de9-9dfb-141a973821ed",
   "metadata": {},
   "source": [
    "We are going to define and detail three encoders: OrdinalEncoder, OneHotEncoder, and LabelEncoder.\n",
    "\n",
    "These are used for converting categorical data into numerical data, which is necessary for most machine learning algorithms.\n",
    "\n",
    "1. **LabelEncoder**:\n",
    "\n",
    "- **Definition**: Transforms each categorical string value into a numerical label. Each unique category is assigned a unique integer from 0 to (number_of_categories - 1).\n",
    "\n",
    "- **Use Case**: Typically used for encoding the target variable (i.e., the `y` in supervised learning). It is not suitable for feature variables (X) because it might imply an ordinal relationship (order) where none exists.\n",
    "\n",
    "- **Example**:\n",
    "\n",
    "- Original: ['cat', 'dog', 'bird']\n",
    "\n",
    "- Encoded: [0, 1, 2]\n",
    "\n",
    "- **Caution**: If used on features, the model might misinterpret the categories as having an order (like 0 < 1 < 2). Therefore, for features, we use OneHotEncoder or OrdinalEncoder appropriately.\n",
    "\n",
    "2. **OrdinalEncoder**:\n",
    "\n",
    "- **Definition**: Similar to LabelEncoder but is designed for feature variables (X). It encodes categorical features as an integer array where each category is assigned a unique integer from 0 to (n_categories - 1). It can handle multiple columns at once.\n",
    "\n",
    "- **Use Case**: When the categorical feature has an inherent ordinal relationship (i.e., the categories have an order). For example, sizes: ['S', 'M', 'L'] can be mapped to [0, 1, 2] because S < M < L.\n",
    "\n",
    "- **Example**:\n",
    "\n",
    "- Input: [['S'], ['M'], ['L'], ['S']]\n",
    "\n",
    "- Output: [[0], [1], [2], [0]]\n",
    "\n",
    "3. **OneHotEncoder**:\n",
    "\n",
    "- **Definition**: Transforms each categorical feature with `n` possible values into `n` binary features (also called dummy variables). Only one of these features is 1 (hot) for each sample, and the rest are 0.\n",
    "\n",
    "- **Use Case**: When the categorical features are nominal (no intrinsic order). For example, colors: ['red', 'blue', 'green'] have no order. Using one-hot encoding avoids the issue of the model assuming an order.\n",
    "\n",
    "- **Example**:\n",
    "\n",
    "- Original: ['red', 'blue', 'green', 'red']\n",
    "\n",
    "- Encoded (as a matrix of binary columns for each category):\n",
    "\n",
    "red:    [1, 0, 0]\n",
    "\n",
    "blue:   [0, 1, 0]\n",
    "\n",
    "green:  [0, 0, 1]\n",
    "\n",
    "red:    [1, 0, 0]\n",
    "\n",
    "- So the full encoded matrix (if we consider columns in order of red, blue, green) is:\n",
    "\n",
    "[[1, 0, 0],\n",
    "\n",
    "[0, 1, 0],\n",
    "\n",
    "[0, 0, 1],\n",
    "\n",
    "[1, 0, 0]]\n",
    "\n",
    "- **Note**: OneHotEncoder can lead to the \"curse of dimensionality\" if there are many categories. In such cases, we might use techniques like feature hashing or embedding.\n",
    "\n",
    "**Key Differences**:\n",
    "\n",
    "- **LabelEncoder** vs **OrdinalEncoder**:\n",
    "\n",
    "- LabelEncoder is for the target variable (y), and OrdinalEncoder is for the input features (X).\n",
    "\n",
    "- LabelEncoder takes a 1D array (like a single column) as input, while OrdinalEncoder can handle 2D data (multiple columns at once).\n",
    "\n",
    "- **OrdinalEncoder** vs **OneHotEncoder**:\n",
    "\n",
    "- OrdinalEncoder assigns integers to categories and is used when there is an order in the categories.\n",
    "\n",
    "- OneHotEncoder creates binary columns for each category and is used when there is no order (nominal data).\n",
    "\n",
    "**When to use which**?\n",
    "\n",
    "- **LabelEncoder**: Only for the target variable (y) in classification problems.\n",
    "\n",
    "- **OrdinalEncoder**: For input features (X) that are ordinal (like ratings: 'low', 'medium', 'high').\n",
    "\n",
    "- **OneHotEncoder**: For input features (X) that are nominal (like countries: 'USA', 'India', 'UK').\n",
    "\n",
    "**Example Code Snippets (using scikit-learn)**:\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "# Example for LabelEncoder (on target y)\n",
    "\n",
    "y = ['cat', 'dog', 'bird', 'dog']\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "y_encoded = le.fit_transform(y)  # [0, 1, 2, 1]\n",
    "\n",
    "# Example for OrdinalEncoder (on feature X with ordinal categories)\n",
    "\n",
    "X_ordinal = [['low'], ['medium'], ['high'], ['medium']]\n",
    "\n",
    "oe = OrdinalEncoder(categories=[['low', 'medium', 'high']])  # specify the order\n",
    "\n",
    "X_ordinal_encoded = oe.fit_transform(X_ordinal)  # [[0.], [1.], [2.], [1.]]\n",
    "\n",
    "# Example for OneHotEncoder (on feature X with nominal categories)\n",
    "\n",
    "X_nominal = [['red'], ['blue'], ['green'], ['red']]\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "X_nominal_encoded = ohe.fit_transform(X_nominal).toarray()\n",
    "\n",
    "# Output (if categories are ['blue','green','red']? note: default is sorted or specified):\n",
    "\n",
    "# [[0,0,1], [1,0,0], [0,1,0], [0,0,1]]\n",
    "\n",
    "# But note: the order of categories can be controlled.\n",
    "\n",
    "# Alternatively, we can use `sparse=False` to get a dense array without .toarray()\n",
    "\n",
    "```\n",
    "\n",
    "**Important Note**:\n",
    "\n",
    "- OneHotEncoder by default returns a sparse matrix (to save memory when there are many categories). We use `.toarray()` to convert to a dense array for visibility. In practice, we might keep it sparse.\n",
    "\n",
    "- For OrdinalEncoder, we can specify the order of categories (as above) to ensure consistency.\n",
    "\n",
    "**Handling Unknown Categories**:\n",
    "\n",
    "- In scikit-learn, we can set `handle_unknown` parameter for `OneHotEncoder` and `OrdinalEncoder` (e.g., to 'ignore' or 'error').\n",
    "\n",
    "**Pandas Alternative**:\n",
    "\n",
    "- For OneHotEncoder, pandas has `pd.get_dummies` which is similar but not exactly the same (it doesn't save the mapping for future data, so for production, use OneHotEncoder in a pipeline).\n",
    "\n",
    "By understanding the nature of the categorical data (target vs feature, ordinal vs nominal), we can choose the appropriate encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72199363-3168-4e3c-9ccb-4b944f22327c",
   "metadata": {},
   "source": [
    "1. OrdinalEncoder\n",
    "\n",
    "Definition:\n",
    "OrdinalEncoder converts categorical features into integer arrays where each unique category is mapped to an integer (0, 1, 2, ...). It preserves ordinal relationships (order) between categories if they exist.\n",
    "\n",
    "Key Characteristics:\n",
    "\n",
    "Input: Multiple feature columns (2D array-like).\n",
    "\n",
    "Output: Integer-encoded matrix (same shape as input).\n",
    "\n",
    "Handles Order: Explicitly defines category order via categories parameter.\n",
    "\n",
    "Use Case: Ordinal data (e.g., \"low\" < \"medium\" < \"high\").\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a5e83-a73d-43c7-bf93-d66b8afcb8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "data = [[\"low\"], [\"medium\"], [\"high\"], [\"medium\"]]\n",
    "encoder = OrdinalEncoder(categories=[[\"low\", \"medium\", \"high\"]])\n",
    "encoded_data = encoder.fit_transform(data)  # Output: [[0.], [1.], [2.], [1.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d4ef6e-fb4a-4cd1-baec-a7547028a9a2",
   "metadata": {},
   "source": [
    "Use Cases:\n",
    "\n",
    "Survey responses (e.g., \"disagree\", \"neutral\", \"agree\").\n",
    "\n",
    "Education levels (e.g., \"high school\", \"bachelor\", \"master\").\n",
    "\n",
    "Size categories (e.g., \"S\", \"M\", \"L\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea6e52f-aa84-457f-adc0-07e05e7ebe70",
   "metadata": {},
   "source": [
    "2. OneHotEncoder\n",
    "\n",
    "Definition:\n",
    "OneHotEncoder converts categorical features into a binary matrix where each category becomes a new binary (0/1) column. Only one column is \"hot\" (1) per sample.\n",
    "\n",
    "Key Characteristics:\n",
    "\n",
    "Input: Multiple feature columns (2D array-like).\n",
    "\n",
    "Output: Sparse or dense binary matrix (shape: [n_samples, n_categories]).\n",
    "\n",
    "Handles Order: Treats categories as nominal (no order).\n",
    "\n",
    "Dummy Variable Trap: Use drop=\"first\" to avoid multicollinearity in linear models.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b8c59-b8d3-484f-89d3-b29896942105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data = [[\"cat\"], [\"dog\"], [\"bird\"], [\"dog\"]]\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_data = encoder.fit_transform(data)\n",
    "# Output (columns: bird, cat, dog):\n",
    "# [[0., 1., 0.],\n",
    "#  [0., 0., 1.],\n",
    "#  [1., 0., 0.],\n",
    "#  [0., 0., 1.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a5bfdc-7d58-4b29-b95d-cded6dab01d1",
   "metadata": {},
   "source": [
    "Use Cases:\n",
    "\n",
    "Nominal data (e.g., colors: \"red\", \"blue\", \"green\").\n",
    "\n",
    "Country names (e.g., \"USA\", \"Japan\", \"Germany\").\n",
    "\n",
    "ID-like categories with no inherent order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f9c324-b24f-4db0-be24-9dfd0dc1458f",
   "metadata": {},
   "source": [
    "3. LabelEncoder\n",
    "\n",
    "Definition:\n",
    "LabelEncoder converts target labels (dependent variables) into integers (0, 1, 2, ...). Designed exclusively for encoding a single target vector.\n",
    "\n",
    "Key Characteristics:\n",
    "\n",
    "Input: 1D array-like (target labels).\n",
    "\n",
    "Output: 1D integer array.\n",
    "\n",
    "Not for Features: Misusing it on features can imply unintended ordinal relationships.\n",
    "\n",
    "Use Case: Encoding class labels in classification.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b1cedc-ec6b-4de9-a5d3-ad745993364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labels = [\"cat\", \"dog\", \"bird\", \"dog\"]\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(labels)  # Output: [0, 1, 2, 1]\n",
    "# Inverse: encoder.inverse_transform([0, 1, 2]) → [\"cat\", \"dog\", \"bird\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0301a17-7bdc-4187-9bc0-560385b17deb",
   "metadata": {},
   "source": [
    "Use Cases:\n",
    "\n",
    "Encoding target variables in classification (e.g., \"spam\"=0, \"not spam\"=1).\n",
    "\n",
    "Never use for input features (use OrdinalEncoder or OneHotEncoder instead)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b226bc-6897-4aa7-9eaa-637575c5092e",
   "metadata": {},
   "source": [
    "When to Use Which\n",
    "OrdinalEncoder:\n",
    "\n",
    "Features with meaningful order (e.g., \"cold\" < \"warm\" < \"hot\").\n",
    "\n",
    "Tree-based models (decision trees, random forests) where order matters.\n",
    "\n",
    "OneHotEncoder:\n",
    "\n",
    "Nominal features with no order (e.g., cities, product IDs).\n",
    "\n",
    "Linear models (logistic regression) to avoid false ordinal assumptions.\n",
    "\n",
    "LabelEncoder:\n",
    "\n",
    "Only for target variables in classification tasks.\n",
    "\n",
    "Never for input features (use OrdinalEncoder if integers are needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea582c6-5a44-4736-bde5-98a2762f2b3d",
   "metadata": {},
   "source": [
    "Best Practices\n",
    "Avoid LabelEncoder for Features: It alphabetically encodes labels, potentially creating false ordinal relationships.\n",
    "\n",
    "Specify Category Order in OrdinalEncoder: Use categories=[[\"low\", \"med\", \"high\"]] to enforce correct ordering.\n",
    "\n",
    "Handle Unknown Categories: Set handle_unknown=\"ignore\" in OneHotEncoder/OrdinalEncoder for test data.\n",
    "\n",
    "Prevent Dummy Trap: For linear models, use OneHotEncoder(drop=\"first\").\n",
    "\n",
    "By choosing the right encoder, you ensure your model correctly interprets categorical data, improving accuracy and avoiding bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0751284-129a-47bb-bc59-bb279115bbe5",
   "metadata": {},
   "source": [
    "<h3 style='color:green;'>Other Encoders</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d373f3d2-ccf0-4cf3-8c5c-eba6d56a7c7d",
   "metadata": {},
   "source": [
    "1. Target Encoding (Mean Encoding)\n",
    "   \n",
    "Purpose: Replaces categories with the mean of the target variable for that category.\n",
    "\n",
    "Mechanism:\n",
    "Category Value = Mean(Target | Category)\n",
    "e.g., For binary classification: P(target=1 | category)\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "High-cardinality features (e.g., ZIP codes, product IDs)\n",
    "\n",
    "Tree-based models (gradient boosting, random forests)\n",
    "\n",
    "Risk: Overfitting (use smoothing or cross-validation)\n",
    "\n",
    "Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b600c52-a8b8-4317-8d17-68be786d4a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "encoder = TargetEncoder()\n",
    "X_encoded = encoder.fit_transform(X_cat, y)  # y = target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbf0412-3a78-41b3-b007-9bfdc6868992",
   "metadata": {},
   "source": [
    "2. Binary Encoding\n",
    "\n",
    "Purpose: Hybrid approach combining ordinal encoding + binary conversion.\n",
    "\n",
    "Mechanism:\n",
    "\n",
    "Convert categories to ordinal integers\n",
    "\n",
    "Represent integers as binary code\n",
    "\n",
    "Split binary digits into separate columns\n",
    "Example: Category 3 → [0, 1, 1] (if using 3 bits)\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "High-cardinality features (reduces dimensionality vs. one-hot)\n",
    "\n",
    "Memory-efficient alternative to one-hot encoding\n",
    "\n",
    "Output: log2(n_categories) columns per feature\n",
    "\n",
    "Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7993a738-de67-4a79-affe-a856532b1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import BinaryEncoder\n",
    "encoder = BinaryEncoder()\n",
    "X_encoded = encoder.fit_transform(X_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c30bb2c-3a08-4885-8d27-b0d0ac38798c",
   "metadata": {},
   "source": [
    "3. Frequency Encoding\n",
    "\n",
    "Purpose: Replaces categories with their occurrence frequency in the dataset.\n",
    "\n",
    "Mechanism:\n",
    "Category Value = Count(category) / Total samples\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "Capturing influential frequent/rare categories\n",
    "\n",
    "Anomaly detection (rare categories may signal outliers)\n",
    "\n",
    "Limitation: Loses category identity information\n",
    "\n",
    "Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69509f19-3cfa-47ce-af5d-51cee180d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = X_cat['feature'].value_counts(normalize=True)\n",
    "X_encoded = X_cat['feature'].map(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fef7a69-a314-47f0-b021-a85e20eb1469",
   "metadata": {},
   "source": [
    "4. Hashing Encoding\n",
    "\n",
    "Purpose: Projects categories into a fixed-dimensional space via hash functions.\n",
    "\n",
    "Mechanism:\n",
    "\n",
    "Uses hash functions (e.g., MD5, MurmurHash) to map categories to fixed buckets\n",
    "\n",
    "Outputs a binary matrix (similar to one-hot but with collisions)\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "Extremely high-cardinality features (e.g., user IDs)\n",
    "\n",
    "Online learning (handles new categories gracefully)\n",
    "\n",
    "Risk: Hash collisions (multiple categories → same bucket)\n",
    "\n",
    "Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87d93b8-9ee1-49aa-bf59-074a98885d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import HashingEncoder\n",
    "encoder = HashingEncoder(n_components=8)  # 8 output columns\n",
    "X_encoded = encoder.fit_transform(X_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bfe1d6-8e85-467b-97ec-53fed7fd4980",
   "metadata": {},
   "source": [
    "5. Leave-One-Out Encoding\n",
    "Purpose: Specialized target encoding that reduces overfitting.\n",
    "\n",
    "Mechanism:\n",
    "For each row:\n",
    "Category Value = Mean(target of other rows in same category)\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "Regression/classification with small datasets\n",
    "\n",
    "When standard target encoding causes leakage\n",
    "\n",
    "Advantage: Minimizes target leakage vs. standard target encoding\n",
    "\n",
    "Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6213a756-8562-47bd-af0c-1b45f23f8e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import LeaveOneOutEncoder\n",
    "encoder = LeaveOneOutEncoder()\n",
    "X_encoded = encoder.fit_transform(X_cat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5be212-8eba-4dad-bee4-7cab2bce1d61",
   "metadata": {},
   "source": [
    "6. Weight of Evidence (WoE)\n",
    "Purpose: Measures predictive power of categories for binary classification.\n",
    "\n",
    "Mechanism:\n",
    "WoE = ln( % of Positives in Category / % of Negatives in Category )\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "Credit risk modeling (finance)\n",
    "\n",
    "Feature selection (high WoE = strong predictor)\n",
    "\n",
    "Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dd72fb-ba6d-43e9-a020-9fb9d9747663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import WOEEncoder\n",
    "encoder = WOEEncoder()\n",
    "X_encoded = encoder.fit_transform(X_cat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faf12c2-e73c-4d50-bee5-b2849053467e",
   "metadata": {},
   "source": [
    "7. Polynomial Encoding\n",
    "Purpose: Creates orthogonal polynomial contrasts for ordered categories.\n",
    "\n",
    "Mechanism:\n",
    "\n",
    "Encodes ordered categories into orthogonal polynomial components (linear, quadratic, cubic)\n",
    "\n",
    "Preserves ordinality without artificial distance assumptions\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "Ordinal features in linear models (ANOVA, regression)\n",
    "\n",
    "Scientific experiments with ordered treatment levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ff054-287a-4bba-85e6-e1118dfd7e1c",
   "metadata": {},
   "source": [
    "Encoder Selection Guide\n",
    "Scenario\t                      Recommended Encoder\n",
    "High-cardinality features\t      Target, Binary, or Hashing\n",
    "Ordinal features (known order)\t  Ordinal or Polynomial\n",
    "Nominal features (no order)\t      OneHot or Hashing\n",
    "Binary classification (finance)\t  WoE\n",
    "Tree-based models\t              Target, Ordinal, or Frequency\n",
    "Linear models\t                  OneHot (with drop) or WoE\n",
    "Preventing target leakage\t      Leave-One-Out\n",
    "Memory-constrained environments\t  Binary or Hashing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb282f8a-3576-4413-8acc-bc84c1074d10",
   "metadata": {},
   "source": [
    "Key Considerations\n",
    "Overfitting Risk: Target-based encoders (WoE, Target, LOO) require careful validation.\n",
    "\n",
    "Dimensionality: One-hot expands dimensions exponentially; prefer binary/hashing for 100+ categories.\n",
    "\n",
    "Business Logic: WoE/Frequency encoders incorporate domain knowledge.\n",
    "\n",
    "Pipeline Integration: Always fit encoders on training data only to avoid data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f877dec-d393-4ba1-be80-d1079f4e1b6f",
   "metadata": {},
   "source": [
    "<h3 style='color:green;'>Simple Imputer</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93b755f-b80a-4c33-abba-0ecf4d6998bf",
   "metadata": {},
   "source": [
    "We are going to explain the Simple Imputer in machine learning, which is a tool for handling missing data.\n",
    "\n",
    "Missing data is a common issue in real-world datasets. The Simple Imputer from scikit-learn provides basic strategies for imputing missing values.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. What is Simple Imputer?\n",
    "\n",
    "2. Why do we need it?\n",
    "\n",
    "3. How does it work? (Strategies)\n",
    "\n",
    "4. How to use it in scikit-learn?\n",
    "\n",
    "5. Important parameters and attributes.\n",
    "\n",
    "6. Example with code (using scikit-learn).\n",
    "\n",
    "7. Considerations and best practices.\n",
    "\n",
    "Let's break it down:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d11cd-47bc-4d13-8763-6367a2eea6d4",
   "metadata": {},
   "source": [
    "Simple Imputer in Machine Learning: A Detailed Explanation\n",
    "1. What is Simple Imputer?\n",
    "Simple Imputer is a preprocessing tool in scikit-learn used to handle missing values (represented as NaN or None) in numerical or categorical data. It replaces missing values with a specified statistic (e.g., mean, median) or a constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766463b2-5fd6-4706-bc5c-63a711630177",
   "metadata": {},
   "source": [
    "2. Why Use Simple Imputer?\n",
    "Real-world data often contains missing values due to errors, omissions, or data collection issues.\n",
    "\n",
    "Most ML algorithms cannot handle missing values (e.g., Scikit-learn models throw errors if fed NaN).\n",
    "\n",
    "Prevents biased/incomplete analysis by filling gaps in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd8b40-b7a8-47cc-8b68-5b5d0781d345",
   "metadata": {},
   "source": [
    "3. Key Strategies for Imputation\n",
    "Simple Imputer supports four primary strategies:\n",
    "\n",
    "Strategy\t          Description\t                      Data Type\n",
    "mean\t        Replace with the feature’s mean.\t    Numerical only\n",
    "median\t        Replace with the feature’s median.\t    Numerical only\n",
    "most_frequent\tReplace with the most frequent value.   Numerical or Categorical\n",
    "constant\t    Replace with a user-defined constant.   All types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736520cf-ddb3-4e50-bb4d-4d9c31e38041",
   "metadata": {},
   "source": [
    "4. Parameters of Simple Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d45977-6963-4cdc-931e-01fbcc141fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(\n",
    "    missing_values=np.nan,  # Values to treat as missing (default: np.nan)\n",
    "    strategy='mean',        # Strategy: 'mean', 'median', 'most_frequent', 'constant'\n",
    "    fill_value=None,        # Used when strategy='constant'\n",
    "    copy=True               # If False, imputation is done in-place\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84b3b86-5033-4beb-a979-fd7407c7b3d2",
   "metadata": {},
   "source": [
    "5. How It Works: Step-by-Step\n",
    "Fit Phase:\n",
    "The imputer calculates the specified statistic (e.g., mean) for each feature in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b1fc0-086a-41d3-a540-a3ff9504aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(X_train)  # Computes statistics per column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8b562d-6964-4f9b-a7da-ecaf2f010614",
   "metadata": {},
   "source": [
    "Transform Phase:\n",
    "Applies the imputation to replace missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dfefeb-7905-4c76-aa22-e3354127539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed = imputer.transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)  # Uses stats from training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ecdb76-a85d-444b-b248-bd3438298b2e",
   "metadata": {},
   "source": [
    "6. Practical Example with Code\n",
    "Dataset:\n",
    "\n",
    "Age\tSalary\n",
    "25\t50000\n",
    "NaN\t54000\n",
    "30\tNaN\n",
    "35\t58000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79d66dd-849b-47b4-923a-c6dbc25c9a67",
   "metadata": {},
   "source": [
    "Step 1: Impute Numerical Data (e.g., Salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295bd496-d1c6-4703-bd9f-eb8ed48db001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Sample data with missing values\n",
    "X = np.array([[25, 50000], [np.nan, 54000], [30, np.nan], [35, 58000]])\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "print(X_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1015abce-2a12-48aa-b486-33c3e47f764a",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ed6e20-5f68-4cb7-a64f-cd8e28064b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[25.  50000.]\n",
    " [30.  54000.]  # Age: mean(25,30,35)=30\n",
    " [30.  54000.]  # Salary: mean(50000,54000,58000)=54000\n",
    " [35.  58000.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f130f-cb97-42af-83c1-7bca25978d70",
   "metadata": {},
   "source": [
    "Step 2: Impute Categorical Data (e.g., Country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9141f354-a9e5-4e2f-828a-7065a86d3e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical data with missing values\n",
    "X_cat = np.array([['Germany'], [np.nan], ['Japan'], ['Germany']])\n",
    "\n",
    "# Impute with the most frequent value\n",
    "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "X_cat_imputed = imputer_cat.fit_transform(X_cat)\n",
    "\n",
    "print(X_cat_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17556d01-4a24-4bb0-a748-d244162b08c7",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f83b3de-730d-421b-bb40-c762f9c311a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "[['Germany']\n",
    " ['Germany']  # Replaced with the mode 'Germany'\n",
    " ['Japan']\n",
    " ['Germany']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6c2927-f079-41b4-9d55-006e413c0c03",
   "metadata": {},
   "source": [
    "7. Important Considerations\n",
    "Avoid Data Leakage:\n",
    "Always fit the imputer on the training data only, then transform the test data using the same statistics.\n",
    "\n",
    "Categorical Data:\n",
    "Use strategy='most_frequent' or strategy='constant' with fill_value.\n",
    "\n",
    "Scaling Sensitivity:\n",
    "Mean/median imputation assumes features are scaled. Use feature scaling (e.g., StandardScaler) after imputation.\n",
    "\n",
    "Impact on Models:\n",
    "\n",
    "Pros: Simple, fast, preserves data structure.\n",
    "\n",
    "Cons: May distort distributions or reduce variance. Advanced techniques (e.g., KNNImputer) are better for complex cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370255e-9dd7-4d28-8345-7c970da9e918",
   "metadata": {},
   "source": [
    "8. Best Practices\n",
    "Exploratory Analysis:\n",
    "Check missingness patterns (e.g., using df.isnull().sum() in pandas).\n",
    "\n",
    "Pipeline Integration:\n",
    "Use in a Pipeline to avoid leakage and ensure reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c66101-2e94-458a-9836-fad7886b0dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74324422-40c3-4646-9bd5-5bed682e3dfe",
   "metadata": {},
   "source": [
    "Combine with Indicator:\n",
    "Add a binary column to mark imputed values (use MissingIndicator in scikit-learn) to signal missingness to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8637bb-504a-4ca5-8e4a-1c92be39c7ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "9. Alternatives to Simple Imputer\n",
    "KNN Imputer: Uses k-nearest neighbors to impute missing values.\n",
    "\n",
    "Iterative Imputer: Models each feature with missing values as a function of other features (more robust but slower)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e3706-bb63-4d20-ae32-e60c64c8860b",
   "metadata": {},
   "source": [
    "Summary\n",
    "Simple Imputer is a fundamental tool for handling missing values by replacing them with statistics/constants.\n",
    "\n",
    "Key Steps: fit() (compute statistics) → transform() (replace missing values).\n",
    "\n",
    "Use Cases: Quick imputation for numerical/categorical data in small-to-medium datasets.\n",
    "\n",
    "Caution: Test different strategies and validate model performance after imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c47b4ad-580d-4c39-911b-14d7395b3563",
   "metadata": {},
   "source": [
    "<h3 style='color:green;'>KNN Imputer and Iterative Imputer.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59f6c0-bc6c-4250-ba8a-bbcf9c2c2384",
   "metadata": {},
   "source": [
    "#### 1. KNN Imputer\n",
    "\n",
    "**What it is**:\n",
    "\n",
    "KNN Imputer is a more advanced technique that uses the k-Nearest Neighbors algorithm to impute missing values. Instead of using a simple statistic (like mean or median) for the entire feature, it uses the values from the most similar instances (neighbors) to fill in the missing data.\n",
    "\n",
    "**How it works**:\n",
    "\n",
    "- For each sample (row) that has a missing value in a feature, the algorithm finds the `k` nearest neighbors (using a distance metric, typically Euclidean distance) that have the feature present.\n",
    "\n",
    "- The missing value is then imputed as the average (for continuous features) or mode (for categorical features) of the corresponding feature in these neighbors.\n",
    "\n",
    "**Key Parameters** (from `sklearn.impute.KNNImputer`):\n",
    "\n",
    "- `n_neighbors`: Number of neighbors to use (default=5).\n",
    "\n",
    "- `weights`: Weighting of neighbors ('uniform' or 'distance').\n",
    "\n",
    "- `metric`: Distance metric (default='nan_euclidean', which can handle missing values).\n",
    "\n",
    "- `copy`: Whether to create a copy of the data.\n",
    "\n",
    "**Advantages**:\n",
    "\n",
    "- Can capture local correlations in the data.\n",
    "\n",
    "- May yield better results than global statistics when the data has clusters or patterns.\n",
    "\n",
    "**Disadvantages**:\n",
    "\n",
    "- Computationally expensive for large datasets (since it requires calculating pairwise distances).\n",
    "\n",
    "- Requires careful choice of `k` (too small: overfitting; too large: underfitting).\n",
    "\n",
    "- The distance metric might not be meaningful for high-dimensional data (curse of dimensionality).\n",
    "\n",
    "**Example Code**:\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Sample data with missing values\n",
    "\n",
    "X = np.array([[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]])\n",
    "\n",
    "# Initialize KNNImputer with 2 neighbors\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "# Fit and transform the data\n",
    "\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "print(X_imputed)\n",
    "\n",
    "```\n",
    "\n",
    "**Output Explanation**:\n",
    "\n",
    "For the first row, the missing value in the third column is imputed by the values from the two nearest neighbors (rows). The two nearest neighbors to [1, 2, ?] are [3,4,3] and [8,8,7] (if we compute Euclidean distance without the missing feature). However, note that the distance is computed using the available features. The imputed value might be the average of 3 and 7 (if they are the two closest) which is 5.\n",
    "\n",
    "#### 2. Iterative Imputer\n",
    "\n",
    "**What it is**:\n",
    "\n",
    "Iterative Imputer is a sophisticated method that models each feature with missing values as a function of other features. It uses a round-robin approach: it iteratively imputes missing values by using the entire set of features to predict missing values. It is based on the idea of Multivariate Imputation by Chained Equations (MICE).\n",
    "\n",
    "**How it works**:\n",
    "\n",
    "- In each iteration, one feature is designated as output and the others as inputs.\n",
    "\n",
    "- A model (e.g., linear regression, BayesianRidge, etc.) is trained to predict the output feature using the input features (only on the samples where the output feature is not missing).\n",
    "\n",
    "- Then, the model is used to predict the missing values in the output feature.\n",
    "\n",
    "- This process cycles through each feature with missing values multiple times (for several iterations) until convergence or a fixed number of iterations.\n",
    "\n",
    "**Key Parameters** (from `sklearn.impute.IterativeImputer`):\n",
    "\n",
    "- `estimator`: The estimator to use for prediction (default=BayesianRidge).\n",
    "\n",
    "- `max_iter`: Maximum number of imputation rounds (default=10).\n",
    "\n",
    "- `tol`: Tolerance for stopping (if the change between iterations is below this, it stops).\n",
    "\n",
    "- `random_state`: For reproducibility.\n",
    "\n",
    "- `initial_strategy`: How to initialize missing values (e.g., 'mean', 'median').\n",
    "\n",
    "**Advantages**:\n",
    "\n",
    "- Very flexible and can model complex relationships.\n",
    "\n",
    "- Often yields more accurate imputations than simple methods.\n",
    "\n",
    "**Disadvantages**:\n",
    "\n",
    "- Computationally very expensive (fits a model for each feature in each iteration).\n",
    "\n",
    "- Requires more tuning (choice of estimator, iterations, etc.).\n",
    "\n",
    "- May be prone to overfitting if not regularized.\n",
    "\n",
    "**Example Code**:\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "\n",
    "X = np.array([[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]])\n",
    "\n",
    "# Initialize IterativeImputer with BayesianRidge estimator\n",
    "\n",
    "imputer = IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=0)\n",
    "\n",
    "# Fit and transform the data\n",
    "\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "print(X_imputed)\n",
    "\n",
    "```\n",
    "\n",
    "**Note**: The `IterativeImputer` is still experimental in scikit-learn, hence we need to import `enable_iterative_imputer` first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a81ba-1b6c-4b7b-a8a1-bfc53f6b3c01",
   "metadata": {},
   "source": [
    "Advanced Alternatives to Simple Imputer\n",
    "While SimpleImputer is efficient for basic imputation, advanced techniques preserve feature relationships and reduce bias. Here are two powerful alternatives:\n",
    "\n",
    "1. KNN Imputer: Neighbor-Based Imputation\n",
    "Concept:\n",
    "Uses the k-Nearest Neighbors algorithm to impute missing values based on feature similarity. Missing values are replaced with the weighted average/mode of the *k* most similar samples.\n",
    "\n",
    "How It Works:\n",
    "\n",
    "For each sample with missing values:\n",
    "\n",
    "Compute distances to all other samples using features without missing values.\n",
    "\n",
    "Identify *k* nearest neighbors.\n",
    "\n",
    "Impute missing values:\n",
    "\n",
    "Numerical: Weighted average of neighbors' values (weights = distance⁻¹).\n",
    "\n",
    "Categorical: Mode of neighbors' values.\n",
    "\n",
    "Key Parameters (sklearn.impute.KNNImputer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaa769b-6cf6-4e34-98aa-c13c28878d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNImputer(\n",
    "    n_neighbors=5,           # Number of neighbors (k)\n",
    "    weights='uniform',       # 'uniform' or 'distance'\n",
    "    metric='nan_euclidean',  # Handles missing values in distance calc\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f34b30-ff17-418e-be58-7ff80de581f2",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19de42f-2a80-4f72-8e95-2630285536a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 2, np.nan], \n",
    "              [3, 4, 3], \n",
    "              [np.nan, 6, 5], \n",
    "              [8, 8, 7]])\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "print(X_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591d5ae6-c8bf-40f4-af76-833e77ce6f5b",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82206d1-8e7d-4fcb-a837-9c70112db83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[1.  2.  4. ]  # NaN replaced by avg of neighbors (3+5)/2=4\n",
    " [3.  4.  3. ]\n",
    " [5.5 6.  5. ]  # NaN replaced by avg of neighbors (3+8)/2=5.5\n",
    " [8.  8.  7. ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e110bc-2969-48ad-83e3-65192f65d447",
   "metadata": {},
   "source": [
    "Pros:\n",
    "\n",
    "Captures local patterns and feature correlations.\n",
    "\n",
    "More accurate than global statistics (mean/median).\n",
    "\n",
    "Cons:\n",
    "\n",
    "Computationally expensive for large datasets (O(n²) complexity).\n",
    "\n",
    "Sensitive to irrelevant features (requires feature selection).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77423a6b-e288-4b42-b60b-c1dca62b9653",
   "metadata": {},
   "source": [
    "2. Iterative Imputer (MICE): Model-Based Imputation\n",
    "Concept:\n",
    "Uses chained equations (Multivariate Imputation by Chained Equations - MICE). Each feature with missing values is modeled as a function of other features, iteratively refining imputations.\n",
    "\n",
    "How It Works:\n",
    "\n",
    "Initialize: Fill missing values with a simple strategy (e.g., mean).\n",
    "\n",
    "Iterate: For each feature with missing values:\n",
    "\n",
    "Treat the feature as a target, other features as predictors.\n",
    "\n",
    "Train a model (e.g., BayesianRidge, RandomForest) on complete cases.\n",
    "\n",
    "Predict missing values using the model.\n",
    "\n",
    "Repeat for max_iter rounds or until convergence.\n",
    "\n",
    "Key Parameters (sklearn.impute.IterativeImputer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14331ae7-4002-4100-8646-50bbbd81b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "IterativeImputer(\n",
    "    estimator=BayesianRidge(),  # Model for prediction\n",
    "    max_iter=10,                # Max iteration rounds\n",
    "    tol=1e-3,                   # Convergence tolerance\n",
    "    random_state=42,\n",
    "    initial_strategy='mean'     # Initial imputation method\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abea89c-25ad-460d-8ab6-0ea923fa6bce",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca631019-e473-4771-9abc-3e7c81255384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "X = np.array([[1, 5, np.nan], \n",
    "              [2, np.nan, 8], \n",
    "              [np.nan, 4, 7], \n",
    "              [4, 7, 8]])\n",
    "\n",
    "imputer = IterativeImputer(estimator=BayesianRidge(), max_iter=10)\n",
    "X_imputed = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cd122a-8e76-4ac5-ac8a-d5cfb9365599",
   "metadata": {},
   "source": [
    "Pros:\n",
    "\n",
    "Models complex feature relationships (non-linearities, interactions).\n",
    "\n",
    "Most accurate for structured data.\n",
    "\n",
    "Cons:\n",
    "\n",
    "Very slow (trains multiple models per feature).\n",
    "\n",
    "Risk of overfitting with noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9de348-58e4-4786-835d-2e575c97c5d8",
   "metadata": {},
   "source": [
    "Key Comparison: Simple vs. KNN vs. Iterative\n",
    "\n",
    "Method\t                Best For\t                         Speed\t      Accuracy\t    Data Relationships\n",
    "Simple Imputer\t    Large datasets,baseline imputation\t  ⚡️ Very Fast\t    Low\t      Ignores relationships\n",
    "KNN Imputer\t        Medium datasets,local patterns\t      ⚠️ Moderate\t    Medium\t  Captures local trends\n",
    "Iterative Imputer\tSmall datasets,complex relationships  🐢 Slow\t        High\t  Models dependencies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
